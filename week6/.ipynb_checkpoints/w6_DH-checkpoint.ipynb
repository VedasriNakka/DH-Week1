{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "governing-irish",
   "metadata": {},
   "source": [
    "### Week6: Digital Humanities\n",
    "#### Group: Jennifer, Vedasri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "circular-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biblical-johns",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary 4 STAR Ibiza Holiday or £10,000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Hope you are having a good week. Just checking in\n",
       "1   ham                            K..give back my thanks.\n",
       "2   ham        Am also doing in cbe only. But have to pay.\n",
       "3  spam  complimentary 4 STAR Ibiza Holiday or £10,000 ...\n",
       "4  spam  okmail: Dear Dave this is your final notice to..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_text = pd.read_csv('corpus/sms_spam.csv', delimiter=\",\")\n",
    "sms_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "earned-fetish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def lower(text):\n",
    "    #sms_text_lower['text'] = sms_text_lower['text'].str.lower()\n",
    "    text = text.str.lower()\n",
    "    #print(\"Length of tokens in sms_text['text'] : {}\".format(len(text_lower['text'])))\n",
    "\n",
    "print(lower(sms_text['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "subject-coral",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lower() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-7c0c25c52597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msms_text_lower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msms_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msms_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(sms_text_lower.head(20))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: lower() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "sms_text_lower = sms_text.copy()\n",
    "sms_text_lower['text'] = sms_text_lower['text'].str.lower()\n",
    "print(\"Length of tokens in sms_text['text'] : {}\".format(len(text_lower['text'])))\n",
    "#print(sms_text_lower.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "significant-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"aim not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \" she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as \",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \" we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \" who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \" why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "#print(contractions.get(\"you have\", \"you have\"))\n",
    "\n",
    "sms_text_lower_updated = sms_text_lower.copy()\n",
    "\n",
    "contractions_array = []\n",
    "for i, line in enumerate(sms_text_lower['text']):\n",
    "    tokens_without_contractions = [contractions.get(word, word) for word in line.split(\" \")]\n",
    "    \n",
    "    sms_text_lower_updated['text'][i] = (\" \").join(tokens_without_contractions)\n",
    "    \n",
    "#sms_text_lower_updated.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "hazardous-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_1 = np.loadtxt('corpus/stopwords.txt', dtype='str')\n",
    "sms_text_only_word_tokens = sms_text_lower_updated.copy()\n",
    "\n",
    "for i,s in enumerate(sms_text_lower['text']):\n",
    "    only_word_tokens = re.findall(\"[a-z]+\", s,re.I)\n",
    "    sms_text_only_word_tokens['text'][i] =  (\" \").join(only_word_tokens) \n",
    "\n",
    "#sms_text_only_word_tokens.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "japanese-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to remove the List of stowords:{} 172.30987548828125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>hope good week checking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>k give back thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>also cbe pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary star ibiza holiday cash needs ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail dear dave final notice collect tenerife...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham                            hope good week checking\n",
       "1   ham                                 k give back thanks\n",
       "2   ham                                       also cbe pay\n",
       "3  spam  complimentary star ibiza holiday cash needs ur...\n",
       "4  spam  okmail dear dave final notice collect tenerife..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "sms_remove_stop_words = sms_text_only_word_tokens.copy()\n",
    "\n",
    "start = time.time()\n",
    "for index,sms in enumerate(sms_text_only_word_tokens['text']):\n",
    "    token_without_sw = [word for word in sms.split(\" \") if not word in stop_words_1]\n",
    "    sms_remove_stop_words['text'][index] = (\" \").join(token_without_sw)\n",
    "    \n",
    "end = time.time()\n",
    "total = end - start\n",
    "print(\"Time taken to remove the List of stowords:{}\",100 *total )\n",
    "\n",
    "sms_remove_stop_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-presence",
   "metadata": {},
   "source": [
    "### Q2. Apply the naïve Bayes to solve the spam detection problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sustained-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Hope you are having a good week. Just checking in\n",
      "1                              K..give back my thanks.\n",
      "2          Am also doing in cbe only. But have to pay.\n",
      "3    complimentary 4 STAR Ibiza Holiday or £10,000 ...\n",
      "4    okmail: Dear Dave this is your final notice to...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0             happened here while you were adventuring\n",
       "1    Ask g or iouri, I've told the story like ten t...\n",
       "2                               Sorry, I'll call later\n",
       "3    Great News! Call FREEFONE 08006344447 to claim...\n",
       "4    Dont know supports srt i thnk. I think ps3 can...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "train = pd.read_csv('corpus/sms_spam.train.csv', delimiter=\",\")\n",
    "train_feature = train.iloc[:,1]\n",
    "train_label = train.iloc[:,0]\n",
    "#clf = GaussianNB()\n",
    "print(train_feature.head(5))\n",
    "\n",
    "test = pd.read_csv('corpus/sms_spam.test.csv', delimiter=\",\")\n",
    "test_feature = test.iloc[:,1]\n",
    "test_label = test.iloc[:,0]\n",
    "test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "solved-trigger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens in sms_text['text'] : 5199\n"
     ]
    }
   ],
   "source": [
    "train_lower = train.copy()\n",
    "train_lower['text'] = train_lower['text'].str.lower()\n",
    "print(\"Length of tokens in sms_text['text'] : {}\".format(len(train_lower['text'])))\n",
    "#print(train_lower.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "scenic-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lower_updated = train_lower.copy()\n",
    "\n",
    "contractions_array = []\n",
    "for i, line in enumerate(train_lower['text']):\n",
    "    tokens_without_contractions_t = [contractions.get(word, word) for word in line.split(\" \")]\n",
    "    \n",
    "    train_lower_updated['text'][i] = (\" \").join(tokens_without_contractions_t)\n",
    "#train_lower_updated.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "compatible-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words_1 = np.loadtxt('corpus/stopwords.txt', dtype='str')\n",
    "train_lower_only_word_tokens = train_lower_updated.copy()\n",
    "\n",
    "for i,s in enumerate(train_lower_updated['text']):\n",
    "    only_word_tokens = re.findall(\"[a-z]+\", s,re.I)\n",
    "    train_lower_only_word_tokens['text'][i] =  (\" \").join(only_word_tokens) \n",
    "\n",
    "#train_lower_only_word_tokens.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "leading-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_remove_stop_words = train_lower_only_word_tokens.copy()\n",
    "\n",
    "for index,sms in enumerate(train_lower_only_word_tokens['text']):\n",
    "    token_without_sw = [word for word in sms.split(\" \") if not word in stop_words_1]\n",
    "    train_remove_stop_words['text'][index] = (\" \").join(token_without_sw)\n",
    "#train_remove_stop_words.head(20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "compressed-toronto",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>hope good week checking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>give back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>cbe pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary star ibiza holiday cash urgent c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail dear dave final notice collect tenerife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>aiya discuss lar pick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>buzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>mummy call father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>marvel mobile play official ultimate spider ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>fyi I usf swing room</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham                            hope good week checking\n",
       "1   ham                                          give back\n",
       "2   ham                                            cbe pay\n",
       "3  spam  complimentary star ibiza holiday cash urgent c...\n",
       "4  spam  okmail dear dave final notice collect tenerife...\n",
       "5   ham                              aiya discuss lar pick\n",
       "6   ham                                               buzy\n",
       "7   ham                                  mummy call father\n",
       "8  spam  marvel mobile play official ultimate spider ma...\n",
       "9   ham                               fyi I usf swing room"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_2 = np.loadtxt('corpus/StopwordSMART.txt', dtype='str')\n",
    "train_remove_stop_words_2 = train_remove_stop_words.copy()\n",
    "\n",
    "for index,sms in enumerate(train_remove_stop_words['text']):\n",
    "    token_without_sw = [word for word in sms.split(\" \") if not word in stop_words_2]\n",
    "    train_remove_stop_words_2['text'][index] = (\" \").join(token_without_sw) \n",
    "\n",
    "train_remove_stop_words_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "arctic-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hams:  4505\n",
      "0    hope good week checking\n",
      "1                  give back\n",
      "2                    cbe pay\n",
      "5      aiya discuss lar pick\n",
      "6                       buzy\n",
      "Name: text, dtype: object\n",
      "Total Spams: 694\n"
     ]
    }
   ],
   "source": [
    "ham_tokens = train_remove_stop_words_2.loc[train_remove_stop_words_2['type'] == 'ham']\n",
    "print(\"Total hams: \",len(ham_tokens))\n",
    "print(ham_tokens['text'].head(5))\n",
    "spam_tokens = train_remove_stop_words_2.loc[train_remove_stop_words_2['type'] == 'spam']\n",
    "print(\"Total Spams:\",len(spam_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cordless-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ur', 'call', 'good', 'day', 'love', 'time', 'home', 'lor', 'da', 'dont', 'today', 'back', 'send', 'pls', 'night', 'hey', 'wat', 'dear', 'happy', 'hope', 'great', 'give', 'work', '', 'yeah', 'make', 'im', 'morning', 'phone']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words_all = []\n",
    "\n",
    "for i, words in enumerate(ham_tokens['text']):\n",
    "    total_words = words.split(\" \")\n",
    "    for w in total_words: \n",
    "        words_all.append(w)\n",
    "        \n",
    "words_dict = Counter(words_all)\n",
    "dict_sorted = {k: v for k, v in sorted(words_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "words_ham_30 = list(dict_sorted.keys())[:30]\n",
    "print(words_ham_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "copyrighted-dividend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'free', 'txt', 'ur', 'stop', 'mobile', 'text', 'claim', 'reply', 'www', 'prize', 'uk', 'send', 'cash', 'win', 'nokia', 'urgent', 'contact', 'msg', 'tone', 'week', 'service', 'box', 'guaranteed', 'customer', 'ppm', 'mins', 'phone', 'cs', 'chat']\n"
     ]
    }
   ],
   "source": [
    "words_all = []\n",
    "\n",
    "for i, words in enumerate(spam_tokens['text']):\n",
    "    total_words = words.split(\" \")\n",
    "    for w in total_words: \n",
    "        words_all.append(w)\n",
    "        \n",
    "words_dict = Counter(words_all)\n",
    "dict_sorted = {k: v for k, v in sorted(words_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "words_spam_30 = list(dict_sorted.keys())[:30]\n",
    "print(words_spam_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-sharing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
