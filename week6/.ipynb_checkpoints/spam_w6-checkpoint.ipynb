{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rough-tennessee",
   "metadata": {},
   "source": [
    "### Week6: Digital Humanities\n",
    "#### Group: Jennifer, Vedasri\n",
    "https://www.kdnuggets.com/2020/07/spam-filter-python-naive-bayes-scratch.html\n",
    "i am usingabove link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "wooden-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "residential-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing shape: (5199, 2)\n",
      "Test shape: (361, 2)\n",
      "   type                                               text\n",
      "0   ham           happened here while you were adventuring\n",
      "1   ham  Ask g or iouri, I've told the story like ten t...\n",
      "2   ham                             Sorry, I'll call later\n",
      "3  spam  Great News! Call FREEFONE 08006344447 to claim...\n",
      "4   ham  Dont know supports srt i thnk. I think ps3 can...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('corpus/sms_spam.train.csv', delimiter=\",\")\n",
    "print(\"Traing shape:\",train.shape)\n",
    "\n",
    "test = pd.read_csv('corpus/sms_spam.test.csv', delimiter=\",\")\n",
    "print(\"Test shape:\",test.shape)\n",
    "print(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "outside-martial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.866513\n",
       "spam    0.133487\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ruled-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.853186\n",
       "spam    0.146814\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "closing-denver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\veda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>hope you are having a good week  just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>k  give back my thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>am also doing in cbe only  but have to pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary 4 star ibiza holiday or  10 000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail  dear dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  hope you are having a good week  just checking in\n",
       "1   ham                            k  give back my thanks \n",
       "2   ham        am also doing in cbe only  but have to pay \n",
       "3  spam  complimentary 4 star ibiza holiday or  10 000 ...\n",
       "4  spam  okmail  dear dave this is your final notice to..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the data\n",
    "train_punctuation = train.copy()\n",
    "train_punctuation['text'] = train['text'].str.replace('\\W', ' ')\n",
    "\n",
    "train_lower = train_punctuation.copy()\n",
    "train_lower['text'] = train_punctuation['text'].str.lower()\n",
    "train_lower.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-environment",
   "metadata": {},
   "source": [
    "### Trying to train on 30 most occurance in ham and 30 spam features. But need to update top 30 keys to main train_lower['text'] column with  type ham & spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "tight-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lower_only_word_tokens = train_lower.copy()\n",
    "\n",
    "for i,s in enumerate(train_lower['text']):\n",
    "    only_word_tokens = re.findall(\"[a-z]+\", s,re.I)\n",
    "    train_lower_only_word_tokens['text'][i] =  (\" \").join(only_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "entire-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop_words\n",
    "stop_words_1 = np.loadtxt('corpus/stopwords.txt', dtype='str')\n",
    "train_remove_stop_words = train_lower_only_word_tokens.copy()\n",
    "\n",
    "for index,sms in enumerate(train_lower_only_word_tokens['text']):\n",
    "    token_without_sw = [word for word in sms.split(\" \") if not word in stop_words_1]\n",
    "    train_remove_stop_words['text'][index] = (\" \").join(token_without_sw)\n",
    "#train_remove_stop_words.head(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "heard-perfume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>hope good week checking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>give back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>cbe pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary star ibiza holiday cash urgent c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail dear dave final notice collect tenerife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>aiya discuss lar pick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>buzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>mummy call father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>marvel mobile play official ultimate spider ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>fyi usf swing room</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham                            hope good week checking\n",
       "1   ham                                          give back\n",
       "2   ham                                            cbe pay\n",
       "3  spam  complimentary star ibiza holiday cash urgent c...\n",
       "4  spam  okmail dear dave final notice collect tenerife...\n",
       "5   ham                              aiya discuss lar pick\n",
       "6   ham                                               buzy\n",
       "7   ham                                  mummy call father\n",
       "8  spam  marvel mobile play official ultimate spider ma...\n",
       "9   ham                                 fyi usf swing room"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_2 = np.loadtxt('corpus/StopwordSMART.txt', dtype='str')\n",
    "train_remove_stop_words_2 = train_remove_stop_words.copy()\n",
    "\n",
    "for index,sms in enumerate(train_remove_stop_words['text']):\n",
    "    token_without_sw = [word for word in sms.split(\" \") if not word in stop_words_2]\n",
    "    train_remove_stop_words_2['text'][index] = (\" \").join(token_without_sw) \n",
    "\n",
    "train_remove_stop_words_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dimensional-raise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hams:  4505\n",
      "0    hope good week checking\n",
      "1                  give back\n",
      "2                    cbe pay\n",
      "5      aiya discuss lar pick\n",
      "6                       buzy\n",
      "Name: text, dtype: object\n",
      "Total Spams: 694\n"
     ]
    }
   ],
   "source": [
    "ham_tokens = train_remove_stop_words_2.loc[train_remove_stop_words_2['type'] == 'ham']\n",
    "print(\"Total hams: \",len(ham_tokens))\n",
    "print(ham_tokens['text'].head(5))\n",
    "spam_tokens = train_remove_stop_words_2.loc[train_remove_stop_words_2['type'] == 'spam']\n",
    "print(\"Total Spams:\",len(spam_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "together-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ur', 'call', 'good', 'day', 'love', 'time', 'home', 'lor', 'da', 'dont', 'today', 'back', 'send', 'pls', 'night', 'hey', 'wat', 'dear', 'happy', 'hope', '', 'great', 'give', 'work', 'yeah', 'make', 'im', 'morning', 'phone', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words_all = []\n",
    "\n",
    "for i, words in enumerate(ham_tokens['text']):\n",
    "    total_words = words.split(\" \")\n",
    "    for w in total_words: \n",
    "        words_all.append(w)\n",
    "        \n",
    "words_dict = Counter(words_all)\n",
    "dict_sorted = {k: v for k, v in sorted(words_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "words_ham_30 = list(dict_sorted.keys())[:30]\n",
    "print(words_ham_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "judicial-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'free', 'txt', 'ur', 'stop', 'mobile', 'text', 'claim', 'reply', 'www', 'prize', 'uk', 'send', 'cash', 'win', 'nokia', 'urgent', 'contact', 'msg', 'tone', 'week', 'service', 'box', 'guaranteed', 'customer', 'ppm', 'mins', 'phone', 'cs', 'chat']\n"
     ]
    }
   ],
   "source": [
    "words_all = []\n",
    "\n",
    "for i, words in enumerate(spam_tokens['text']):\n",
    "    total_words = words.split(\" \")\n",
    "    for w in total_words: \n",
    "        words_all.append(w)\n",
    "        \n",
    "words_dict = Counter(words_all)\n",
    "dict_sorted = {k: v for k, v in sorted(words_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "words_spam_30 = list(dict_sorted.keys())[:30]\n",
    "print(words_spam_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-madison",
   "metadata": {},
   "source": [
    "### Train on total sms_text data set in my system unable to split spam ham rows to spammessages, hammessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "respiratory-thanks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8384"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the vocabulary\n",
    "\n",
    "#train_split = train_lower.copy()\n",
    "#train_split['text'] = train_lower['text'].str.split()\n",
    "#print(len(train_lower))\n",
    "# vocabulary = []\n",
    "\n",
    "# for index, sms in enumerate(train_lower['text']):\n",
    "#     total_words = [vocabulary.append(w) for w in sms.split(\" \")]\n",
    "        \n",
    "# vocabulary = list(set(vocabulary))\n",
    "# len(vocabulary)\n",
    "\n",
    "train_lower['text'] = train_lower['text'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for sms in train_lower['text']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "unlikely-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word counts per test\n",
    "\n",
    "word_counts_per_sms = {unique_word: [0] * len(train_lower['text']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(train_lower['text']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "nominated-phrase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>07821230901</th>\n",
       "      <th>september</th>\n",
       "      <th>paracetamol</th>\n",
       "      <th>hug</th>\n",
       "      <th>everyday</th>\n",
       "      <th>shant</th>\n",
       "      <th>brain</th>\n",
       "      <th>thecd</th>\n",
       "      <th>throwin</th>\n",
       "      <th>bb</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>selflessness</th>\n",
       "      <th>unknown</th>\n",
       "      <th>fuuuuck</th>\n",
       "      <th>habba</th>\n",
       "      <th>resuming</th>\n",
       "      <th>leo</th>\n",
       "      <th>fa</th>\n",
       "      <th>3100</th>\n",
       "      <th>laughing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   07821230901  september  paracetamol  hug  everyday  shant  brain  thecd  \\\n",
       "0            0          0            0    0         0      0      0      0   \n",
       "1            0          0            0    0         0      0      0      0   \n",
       "2            0          0            0    0         0      0      0      0   \n",
       "3            0          0            0    0         0      0      0      0   \n",
       "4            0          0            0    0         0      0      0      0   \n",
       "\n",
       "   throwin  bb  ...  5  selflessness  unknown  fuuuuck  habba  resuming  leo  \\\n",
       "0        0   0  ...  0             0        0        0      0         0    0   \n",
       "1        0   0  ...  0             0        0        0      0         0    0   \n",
       "2        0   0  ...  0             0        0        0      0         0    0   \n",
       "3        0   0  ...  0             0        0        0      0         0    0   \n",
       "4        0   0  ...  0             0        0        0      0         0    0   \n",
       "\n",
       "   fa  3100  laughing  \n",
       "0   0     0         0  \n",
       "1   0     0         0  \n",
       "2   0     0         0  \n",
       "3   0     0         0  \n",
       "4   0     0         0  \n",
       "\n",
       "[5 rows x 8384 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation of training set\n",
    "\n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "otherwise-contrast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5199, 8386)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatinating Label, text to word-counts\n",
    "\n",
    "training_set_clean = pd.concat([train_lower, word_counts], axis=1)\n",
    "training_set_clean.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages first\n",
    "# idx = training_set_clean.to_frame()\n",
    "# training_set_clean.loc[idx.iloc[:,0]]\n",
    "spam_messages = training_set_clean[training_set_clean['type'] == 'spam']\n",
    "ham_messages = training_set_clean[training_set_clean['type'] == 'ham']\n",
    "spam_messages.head(5)\n",
    "# P(Spam) and P(Ham)\n",
    "# p_spam = len(spam_messages) / len(training_set_clean)\n",
    "# p_ham = len(ham_messages) / len(training_set_clean)\n",
    "\n",
    "# # N_Spam\n",
    "# n_words_per_spam_message = spam_messages['text'].apply(len)\n",
    "# n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "# # N_Ham\n",
    "# n_words_per_ham_message = ham_messages['text'].apply(len)\n",
    "# n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "# # N_Vocabulary\n",
    "# n_vocabulary = len(vocabulary)\n",
    "\n",
    "# # Laplace smoothing\n",
    "# alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum() # spam_messages already defined\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "    n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-empire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
