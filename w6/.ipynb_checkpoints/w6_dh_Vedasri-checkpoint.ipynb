{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understood-district",
   "metadata": {},
   "source": [
    "### Week6: Digital Humanities - Vector Space Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pleasant-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lxml.etree\n",
    "import tarfile\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "tf = tarfile.open('theatre-classique.tar.gz','r')\n",
    "tf.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bearing-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgenres = ('Comédie', 'Tragédie', 'Tragi-comédie')\n",
    "#print(subgenres)\n",
    "plays, titles, genres = [], [], []\n",
    "authors, years = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colored-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in os.scandir('data/theatre-classique'):\n",
    "    # Only include XML files\n",
    "    if not fn.name.endswith('.xml'):\n",
    "        continue\n",
    "    tree   = lxml.etree.parse(fn.path)\n",
    "    genre  = tree.find('//genre')\n",
    "    title  = tree.find('//title')\n",
    "    author = tree.find('//author')\n",
    "    year   = tree.find('//date')\n",
    "    if genre is not None and genre.text in subgenres:\n",
    "        lines = []\n",
    "        for line in tree.xpath('//l|//p'):\n",
    "            lines.append(' '.join(line.itertext()))\n",
    "        text = '\\n'.join(lines)\n",
    "        plays.append(text)\n",
    "        genres.append(genre.text)\n",
    "        titles.append(title.text)\n",
    "        authors.append(author.text)\n",
    "        if year is not None:\n",
    "            years.append(year.text)\n",
    "\n",
    "plays = np.array(plays)\n",
    "genres = np.array(genres)\n",
    "titles = np.array(titles)\n",
    "authors = np.array(authors)\n",
    "years = np.array(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intensive-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498 498 498 498 208\n"
     ]
    }
   ],
   "source": [
    "print(len(plays), len(genres), len(titles), len(authors), len(years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occupied-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Veda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sacred-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punct(string):\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\n",
    "       punctuation markers.\n",
    "    \"\"\"\n",
    "    return PUNCT_RE.match(string) is not None\n",
    "\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "premier-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language='French', lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if (language == 'French'):\n",
    "        text = re.sub(\"-\", \" \", text)\n",
    "        text = re.sub(\"l'\", \"le \", text)\n",
    "        text = re.sub(\"d'\", \"de \", text)\n",
    "        text = re.sub(\"c'\", \"ce \", text)\n",
    "        text = re.sub(\"j'\", \"je \", text)\n",
    "        text = re.sub(\"m'\", \"me \", text)\n",
    "        text = re.sub(\"qu'\", \"que \", text)\n",
    "        text = re.sub(\"'\", \" ' \", text)\n",
    "        text = re.sub(\"quelqu'\", \"quelque \", text)\n",
    "        text = re.sub(\"aujourd'hui\", \"aujourdhui\", text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surface-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_tok = [preprocess_text(play, 'French') for play in plays]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "great-sarah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  38410\n"
     ]
    }
   ],
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "    vocabulary = collections.Counter()\n",
    "    for document in tokenized_corpus:\n",
    "        vocabulary.update(document)\n",
    "    vocabulary = {word for word, count in vocabulary.items()\n",
    "                  if count >= min_count and count <= max_count}\n",
    "    return sorted(vocabulary)\n",
    "\n",
    "vocabulary = extract_vocabulary(plays_tok, min_count=2)\n",
    "print(\"Length of vocabulary: \",len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa50b9",
   "metadata": {},
   "source": [
    "#### 1. Represent each play by a vector with only the tf component. You can apply some preprocessing before generating this vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "social-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus2dtm(tokenized_corpus, vocabulary):\n",
    "    document_term_matrix = []\n",
    "    for document in tokenized_corpus:\n",
    "        document_counts = collections.Counter(document)\n",
    "        row = [document_counts[word] for word in vocabulary]\n",
    "        document_term_matrix.append(row)\n",
    "    return np.array(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "specific-andrews",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix with |D| = 498 documents and |V| = 38410 words.\n"
     ]
    }
   ],
   "source": [
    "document_term_matrix = np.array(corpus2dtm(plays_tok, vocabulary))\n",
    "print(f\"document-term matrix with \"\n",
    "      f\"|D| = {document_term_matrix.shape[0]} documents and \"\n",
    "      f\"|V| = {document_term_matrix.shape[1]} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brutal-looking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted doc into vectors :\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Length of matrix: \n",
      " 498\n",
      "\n",
      "Size of matrix: \n",
      " (498, 38410)\n"
     ]
    }
   ],
   "source": [
    "print(\"Converted doc into vectors :\\n\",document_term_matrix)\n",
    "print(\"\\nLength of matrix: \\n\", len(document_term_matrix))\n",
    "print(\"\\nSize of matrix: \\n\", document_term_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70bed1",
   "metadata": {},
   "source": [
    "#### 2. For each genre, it is possible to generate a “profile”, in the form of a single vector representing the entire set of plays corresponding to this genre. Build such a profile for each of the three genres (Comedy, Tragedy and Tragicomedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flying-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_means = document_term_matrix[np.array(genres) == 'Tragédie'].mean(axis=0)\n",
    "co_means = document_term_matrix[genres == 'Comédie'].mean(axis=0)\n",
    "tc_means = document_term_matrix[genres == 'Tragi-comédie'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf007f2a",
   "metadata": {},
   "source": [
    "#### 3. How many terms with a weight strictly larger than 0 do you have in each text genre profile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43603c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb673d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
